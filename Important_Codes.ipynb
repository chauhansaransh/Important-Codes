{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Important Codes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7w5Jvx49-lp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_corr(df,size=10):\n",
        "    '''Function plots a graphical correlation matrix for each pair of columns in the dataframe.\n",
        "\n",
        "    Input:\n",
        "        df: pandas DataFrame\n",
        "        size: vertical and horizontal size of the plot'''\n",
        "\n",
        "    corr = df.corr()\n",
        "    fig, ax = plt.subplots(figsize=(size, size))\n",
        "    ax.legend()\n",
        "    cax = ax.matshow(corr)\n",
        "    fig.colorbar(cax)\n",
        "    plt.xticks(range(len(corr.columns)), corr.columns, rotation='vertical')\n",
        "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
        "    \n",
        "plot_corr(main_df_final)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38XPXAXOBRcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To support both python 2 and python 3\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"end_to_end_project\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    try:\n",
        "        plt.savefig(path, format=fig_extension, dpi=resolution)\n",
        "    except:\n",
        "        plt.savefig(fig_id + \".\" + fig_extension, format=fig_extension, dpi=resolution)\n",
        "\n",
        "# Ignore useless warnings (see SciPy issue #5998)\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
        "pd.options.display.max_columns = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0hQ0WqHAXWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (\"Rows     : \" , insurance.shape[0])\n",
        "print (\"Columns  : \" , insurance.shape[1])\n",
        "print (\"\\nFeatures : \\n\" , insurance.columns.tolist())\n",
        "print (\"\\nMissing values :  \", insurance.isnull().sum().values.sum())\n",
        "print (\"\\nUnique values :  \\n\",insurance.nunique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0_yn7oJ-Sep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#customer id col\n",
        "Id_col = ['customerID']\n",
        "\n",
        "#Target columns\n",
        "target_col = [\"Churn\"]\n",
        "\n",
        "#categorical columns\n",
        "cat_cols = churn_df.nunique()[churn_df.nunique() < 6].keys().tolist()\n",
        "cat_cols = [x for x in cat_cols if x not in target_col]\n",
        "\n",
        "#numerical columns\n",
        "num_cols = [x for x in churn_df.columns if x not in cat_cols + target_col + Id_col]\n",
        "\n",
        "#Binary columns with 2 values\n",
        "bin_cols = churn_df.nunique()[churn_df.nunique() == 2].keys().tolist()\n",
        "\n",
        "#Columns more than 2 values\n",
        "multi_cols = [i for i in cat_cols if i not in bin_cols]\n",
        "\n",
        "#Label encoding Binary columns\n",
        "le = LabelEncoder()\n",
        "for i in bin_cols :\n",
        "    churn_df[i] = le.fit_transform(churn_df[i])\n",
        "    \n",
        "#Duplicating columns for multi value columns\n",
        "churn_df = pd.get_dummies(data = churn_df, columns = multi_cols )\n",
        "churn_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9UUjBO8-7lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvlFpSn3C2MW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from datetime import datetime\n",
        "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "\n",
        "# Receive numpy array, convert to pandas for features, convert back to array for output.\n",
        "\n",
        "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, popularity = True, num_cols=[]): # no *args or **kargs\n",
        "        self.popularity = popularity\n",
        "    def fit(self, X, y=None):\n",
        "        return self  # nothing else to do\n",
        "    def transform(self, X, y=None):\n",
        "        \n",
        "        ### Some feature engineering\n",
        "        X = pd.DataFrame(X, columns=num_cols)\n",
        "        X[\"bedrooms_per_person\"] = X[\"bedrooms\"]/X[\"accommodates\"]\n",
        "        X[\"bathrooms_per_person\"] = X[\"bathrooms\"]/X[\"accommodates\"]\n",
        "        \n",
        "        global feats\n",
        "        feats = [\"bedrooms_per_person\",\"bathrooms_per_person\"]\n",
        "\n",
        "        if self.popularity:\n",
        "            X[\"past_and_future_popularity\"]=X[\"number_of_reviews\"]/(X[\"availability_365\"]+1)\n",
        "            feats.append(\"past_and_future_popularity\")\n",
        "            \n",
        "            return X.values\n",
        "        else:\n",
        "            return X.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LMzG68zDAg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "strat_train_set2 = strat_train_set.dropna()\n",
        "X = strat_train_set2.copy().drop(\"price\",axis=1)\n",
        "Y = strat_train_set2[\"price\"]\n",
        "\n",
        "num_cols = list(X.select_dtypes(include=numerics).columns)\n",
        "cat_cols = list(X.select_dtypes(include=[object]).columns)\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "        ('imputer', Imputer(strategy=\"median\")),\n",
        "        ('attribs_adder', CombinedAttributesAdder(num_cols=num_cols,popularity=True)),\n",
        "        ('std_scaler', StandardScaler()),\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BanGfQbgDSeU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "import itertools\n",
        "\n",
        "\n",
        "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "\n",
        "mid_pipeline = ColumnTransformer([\n",
        "        (\"num\", num_pipeline, num_cols),\n",
        "        (\"cat\", OneHotEncoder(),cat_cols ),\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33HzcQhWDTkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mid_pipeline.fit(X) # this one specifically has to be fitted for the cat names\n",
        "cat_encoder = mid_pipeline.named_transformers_[\"cat\"]\n",
        "sublists = [list(bas) for bas in cat_encoder.categories_]\n",
        "one_cols = list(itertools.chain(*sublists))\n",
        "\n",
        "## In this class, I will be converting numpy back to pandas\n",
        "\n",
        "class ToPandasDF(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, fit_index = [] ): # no *args or **kargs\n",
        "        self.fit_index = fit_index\n",
        "    def fit(self, X_df, y=None):\n",
        "        return self  # nothing else to do\n",
        "    def transform(self, X_df, y=None):\n",
        "        global cols\n",
        "        cols = num_cols.copy()\n",
        "        cols.extend(feats)\n",
        "        cols.extend(one_cols) # one in place of cat\n",
        "        X_df = pd.DataFrame(X_df, columns=cols,index=self.fit_index)\n",
        "\n",
        "        return X_df\n",
        "\n",
        "def pipe(inds):\n",
        "    return Pipeline([\n",
        "            (\"mid\", mid_pipeline),\n",
        "            (\"PD\", ToPandasDF(inds)),\n",
        "        ])\n",
        "    \n",
        "params = {\"inds\" : list(X.index)}\n",
        "\n",
        "X_pr = pipe(**params).fit_transform(X) # Now we have done all the preprocessing instead of\n",
        "                                   #.. doing it bit by bit. The pipeline becomes \n",
        "                                   #.. extremely handy in the cross-validation step."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpm8MWVd0CZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_client = pipe(list(df_client.index)).transform(df_client)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWTNb02gEaXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# We remove the label values from our training data\n",
        "X = hr_df_final.drop(['Survived'],axis=1).values\n",
        "\n",
        "# We assigned those label values to our Y dataset\n",
        "y = hr_df_final['Survived'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-DVgLkLE1S-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "score = model.score(X_test, y_test)\n",
        "\n",
        "print(\"Accuracy {0:.2f}%\".format(100*accuracy_score(predictions, y_test)))\n",
        "print(confusion_matrix(y_test, predictions))\n",
        "print(classification_report(y_test, predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0Vi_IIIFF7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x \n",
        "\n",
        "# If you wish to use Tensorflow 1.X run the following line and then restart runtime\n",
        "# %tensorflow_version 1.x \n",
        "# You'll need to change your import statements from tensorflow.keras to keras\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(18, kernel_initializer = \"uniform\", activation = \"relu\", input_dim=16))\n",
        "model.add(Dense(1, kernel_initializer = \"uniform\", activation = \"sigmoid\"))\n",
        "\n",
        "model.compile(optimizer= \"adam\",loss = \"binary_crossentropy\",metrics = [\"accuracy\"])\n",
        "\n",
        "# Display Model Summary and Show Parameters\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaZ5D817FLlm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start Training Our Classifier \n",
        "batch_size = 10\n",
        "epochs = 50\n",
        "\n",
        "history = model.fit(X_train,\n",
        "                    y_train,\n",
        "                    batch_size = batch_size,\n",
        "                    epochs = epochs,\n",
        "                    verbose = 1,\n",
        "                    )\n",
        "\n",
        "predictions1 = model.predict(X_test)\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE6cNcjcFbiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting our loss charts\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "line1 = plt.plot(epochs, val_loss_values, label='Validation/Test Loss')\n",
        "line2 = plt.plot(epochs, loss_values, label='Training Loss')\n",
        "plt.setp(line1, linewidth=2.0, marker = '+', markersize=10.0)\n",
        "plt.setp(line2, linewidth=2.0, marker = '4', markersize=10.0)\n",
        "plt.xlabel('Epochs') \n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plotting our accuracy charts\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "line1 = plt.plot(epochs, val_acc_values, label='Validation/Test Accuracy')\n",
        "line2 = plt.plot(epochs, acc_values, label='Training Accuracy')\n",
        "plt.setp(line1, linewidth=2.0, marker = '+', markersize=10.0)\n",
        "plt.setp(line2, linewidth=2.0, marker = '4', markersize=10.0)\n",
        "plt.xlabel('Epochs') \n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "predictions = (predictions > 0.5)\n",
        "\n",
        "print(confusion_matrix(y_test, predictions))\n",
        "print(classification_report(y_test, predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni16cS-YLF1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(DecisionTreeRegressor(random_state=42), X_pr, Y,\n",
        "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
        "tree_rmse_scores = np.sqrt(-scores)\n",
        "\n",
        "def display_scores(scores):\n",
        "    print(\"Scores:\", scores)\n",
        "    print(\"Mean:\", scores.mean())\n",
        "    print(\"Standard deviation:\", scores.std())\n",
        "\n",
        "display_scores(tree_rmse_scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THrxjWTsLRyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 30 Seconds to run this code block.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = [\n",
        "    # try 12 (3×4) combinations of hyperparameters\n",
        "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
        "    # then try 6 (2×3) combinations with bootstrap set as False\n",
        "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
        "  ]\n",
        "\n",
        "forest_reg = RandomForestRegressor(random_state=42)\n",
        "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
        "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
        "                           scoring='neg_mean_squared_error', return_train_score=True)\n",
        "grid_search.fit( X_pr, Y)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4gTTzHiLdce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_search.best_params_\n",
        "final_model = grid_search.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWA2_EzJLoZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cvres = grid_search.cv_results_\n",
        "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
        "    print(np.sqrt(-mean_score), params)\n",
        "    \n",
        "print(\"\")\n",
        "print(\"Best grid-search performance: \", np.sqrt(-cvres[\"mean_test_score\"].max()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEbacpXTLy7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "param_distribs = {\n",
        "        'n_estimators': randint(low=1, high=200),\n",
        "        'max_features': randint(low=1, high=8),\n",
        "    }\n",
        "\n",
        "forest_reg = RandomForestRegressor(random_state=42)\n",
        "rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n",
        "                                n_iter=5, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
        "rnd_search.fit( X_pr, Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-d1m6EhL6Hb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cvres = rnd_search.cv_results_\n",
        "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
        "    print(np.sqrt(-mean_score), params)\n",
        "    \n",
        "print(\"Best grid-search performance: \", np.sqrt(-cvres[\"mean_test_score\"].max()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBjZXCq5Mr13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}